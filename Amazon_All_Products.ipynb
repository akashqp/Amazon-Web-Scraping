{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d879fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the scraped data\n",
    "product_names = []\n",
    "prices = []\n",
    "ratings = []\n",
    "sellers = []\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8904bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape product details from a given url\n",
    "def scrape_products(url):\n",
    "    # Send a GET Request to the url\n",
    "    response = requests.get(url, headers = headers)\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Find all the product items on the page\n",
    "    product_items = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    # Iterate over each product item\n",
    "    for item in product_items:\n",
    "\n",
    "        # Extract product name\n",
    "        product_name = item.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}).text.strip()\n",
    "        \n",
    "        # Extract rating\n",
    "        rating = item.find('span', {'class': 'a-icon-alt'})\n",
    "        rating = rating.text.strip() if rating is not None else \"N/A\"\n",
    "        # Extract href and create complete link of the product\n",
    "        href = item.find('a', {'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "        link = 'https://www.amazon.in' + href['href']\n",
    "\n",
    "        # Send a GET request to the product link\n",
    "        response = requests.get(link, headers=headers)\n",
    "        soup1 = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Check if the product is out of stock and continue to next iteration if it is\n",
    "        out_of_stock = soup1.find('div', {'id': 'outOfStock'})\n",
    "        if out_of_stock is not None:\n",
    "            continue\n",
    "\n",
    "        # Extract price\n",
    "        price = item.find('span', {'class': 'a-price-whole'})\n",
    "        price = price.text.strip() if price is not None else \"N/A\"\n",
    "        \n",
    "        # Extract seller name\n",
    "        merchant_info = soup1.find('div', {'id': 'merchant-info'})\n",
    "        seller_link = merchant_info.find('a') if merchant_info is not None else None\n",
    "        seller_name = seller_link.text.strip() if seller_link is not None else \"N/A\"\n",
    "        \n",
    "        if product_name and rating and price and seller_name:\n",
    "            product_names.append(product_name)\n",
    "            ratings.append(rating)\n",
    "            prices.append(price)\n",
    "            sellers.append(seller_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d269ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the first page of search results\n",
    "url = \"https://www.amazon.in/s?rh=n%3A6612025031&fs=true&ref=lp_6612025031_sar\"\n",
    "\n",
    "# Scrape products from the first page\n",
    "scrape_products(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2337ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET Request to the url\n",
    "response = requests.get(url, headers=headers)\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Check if there are multiple pages of search results\n",
    "pagination = soup.find('span', {'class': 's-pagination-item s-pagination-diasbled'})\n",
    "if pagination is None:\n",
    "    # Extract the total number of pages\n",
    "    page_count = int(soup.find('span', {'class': 's-pagination-item s-pagination-disabled'}).text.strip())\n",
    "    \n",
    "    # Scrape Products from the remaining pages\n",
    "    for page_number in range(2, page_count + 1):\n",
    "        page_url = url + f\"&page={page_number}\"\n",
    "        scrape_products(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Product Name': product_names[:1947],\n",
    "    'Price': prices[:1947],\n",
    "    'Rating': ratings[:1947],\n",
    "    'Seller Name': sellers[:1947]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "df.to_csv('amazon_products.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
